# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eg-AsUzRSSAVPaPci4hdFMupQ2XWf3tj

#GAN Implementation
"""

!pip install torch torchvision matplotlib

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load the dataset
batch_size = 64
train_loader = torch.utils.data.DataLoader(
    torchvision.datasets.MNIST('.', train=True, download=True, transform=transform),
    batch_size=batch_size, shuffle=True)

class Generator(nn.Module):
    def __init__(self, input_dim=100, output_dim=784):
        super(Generator, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, output_dim),
            nn.Tanh()
        )

    def forward(self, x):
        return self.fc(x)

class Discriminator(nn.Module):
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.fc(x)

z_dim = 100  # Dimension of the noise vector
generator = Generator(input_dim=z_dim).to(device)
discriminator = Discriminator().to(device)

# Optimizers
lr = 0.0002
G_optimizer = optim.Adam(generator.parameters(), lr=lr)
D_optimizer = optim.Adam(discriminator.parameters(), lr=lr)

# Loss function
criterion = nn.BCELoss()

def train_discriminator(real_images, fake_images):
    # Train on real images
    D_optimizer.zero_grad()
    real_labels = torch.ones(real_images.size(0), 1).to(device)
    real_loss = criterion(discriminator(real_images), real_labels)

    # Train on fake images
    fake_labels = torch.zeros(fake_images.size(0), 1).to(device)
    fake_loss = criterion(discriminator(fake_images), fake_labels)

    # Backprop and optimize
    D_loss = real_loss + fake_loss
    D_loss.backward()
    D_optimizer.step()

    return D_loss.item()

def train_generator(fake_images):
    # Train generator to make discriminator classify fake images as real
    G_optimizer.zero_grad()
    labels = torch.ones(fake_images.size(0), 1).to(device)
    G_loss = criterion(discriminator(fake_images), labels)

    # Backprop and optimize
    G_loss.backward()
    G_optimizer.step()

    return G_loss.item()

# Training parameters
epochs = 100
sample_interval = 10
D_losses, G_losses = [], []

for epoch in range(epochs):
    D_loss_epoch, G_loss_epoch = 0, 0
    for batch_idx, (real_images, _) in enumerate(train_loader):
        # Flatten real images
        real_images = real_images.view(-1, 784).to(device)

        # Generate fake images
        z = torch.randn(batch_size, z_dim).to(device)
        fake_images = generator(z)

        # Train discriminator and generator
        D_loss_epoch += train_discriminator(real_images, fake_images.detach())
        G_loss_epoch += train_generator(fake_images)

    # Calculate average losses for the epoch
    D_losses.append(D_loss_epoch / len(train_loader))
    G_losses.append(G_loss_epoch / len(train_loader))

    # Print progress
    print(f"Epoch [{epoch+1}/{epochs}] | D Loss: {D_losses[-1]:.4f} | G Loss: {G_losses[-1]:.4f}")

    # Save samples at intervals
    if (epoch + 1) % sample_interval == 0:
        with torch.no_grad():
            z = torch.randn(16, z_dim).to(device)
            generated_images = generator(z).view(-1, 1, 28, 28)
            grid_img = torchvision.utils.make_grid(generated_images, nrow=4, normalize=True)
            plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())
            plt.title(f"Epoch {epoch+1}")
            plt.show()

plt.figure(figsize=(10, 5))
plt.plot(D_losses, label="Discriminator Loss")
plt.plot(G_losses, label="Generator Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

""" # Latent Space Interpolation in GANs"""

import numpy as np

def interpolate(z1, z2, steps=10):
    """Generate a series of interpolated latent vectors between z1 and z2."""
    return [(1 - alpha) * z1 + alpha * z2 for alpha in np.linspace(0, 1, steps)]

# Function to visualize interpolated images
def visualize_interpolation(generator, z_dim, steps=10):
    # Generate two random latent vectors
    z1 = torch.randn(1, z_dim).to(device)
    z2 = torch.randn(1, z_dim).to(device)

    # Interpolate between z1 and z2
    interpolated_latents = interpolate(z1, z2, steps=steps)

    # Generate images from each interpolated latent vector
    generated_images = [generator(latent).view(1, 28, 28).detach().cpu() for latent in interpolated_latents]

    # Plot the interpolated images
    fig, axes = plt.subplots(1, steps, figsize=(15, 15))
    for i, img in enumerate(generated_images):
        img = img * 0.5 + 0.5  # Rescale image to [0, 1] for visualization
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].axis("off")
    plt.show()

# After training, visualize interpolation between two generated images
visualize_interpolation(generator, z_dim=100, steps=10)

